{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88c6cc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import glob\n",
    "import gzip\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import h5py\n",
    "delay=0\n",
    "n = 300 # number of batches per test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4b8c57",
   "metadata": {},
   "source": [
    "# Default Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df984cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventsData(Dataset):\n",
    "    def __init__(self,data_dir,per=100):\n",
    "        # Save the directory of the data\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "        # Get the names of the files\n",
    "        self.names=glob.glob(str(data_dir)+'*.hdf5')\n",
    "        # Number of files in the dir\n",
    "        self.size_dir=len(self.names)\n",
    "        \n",
    "        # Set the files acording to the percentage\n",
    "        self.size_dir=math.ceil(len(self.names)*(per/100))\n",
    "        self.names=self.names[0:self.size_dir]\n",
    "        #random.shuffle(self.names)\n",
    "        \n",
    "        # Get the number of events per file\n",
    "        print(self.names[1])\n",
    "        f = h5py.File(self.names[0],'r')\n",
    "        self.size_file=f['y'].shape[0]\n",
    "        \n",
    "        # Get the total number of events\n",
    "        self.total_events=0\n",
    "        for name in self.names:\n",
    "            f = h5py.File(name,'r')\n",
    "            y = f['y']\n",
    "            self.total_events+=y.shape[0]\n",
    "            \n",
    "        # Number of iterations to finish the dataset\n",
    "    \n",
    "       \n",
    "\n",
    "        print(\"There are \"+str(self.total_events)+\" events.\")\n",
    "        print(\"In \"+str(self.size_dir)+\" separate files.\")\n",
    "        print(\"Each file containing \"+str(self.size_file)+\" events.\")\n",
    "       \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_events\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the file that shall be opened\n",
    "        file=math.floor(idx/self.size_file)\n",
    "        new_idx=idx-file*self.size_file\n",
    "        f = h5py.File(self.names[file],'r')\n",
    "        \n",
    "        \n",
    "        data=torch.tensor(f['X1'][new_idx,:,:]).unsqueeze(dim=3)\n",
    "        \n",
    "        #Only for conv with modulus\n",
    "        data=data.transpose(1,3)\n",
    "        data=data.transpose(2,3)\n",
    "        \n",
    "        target=torch.tensor(f['y'][new_idx,0])\n",
    "        target=torch.arccos(target)\n",
    "    \n",
    "        \n",
    "        return data.float(),target.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99d8348e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mod_full_dist/train_data_fixed/train_dataset_42.hdf5\n",
      "There are 431000 events.\n",
      "In 431 separate files.\n",
      "Each file containing 1000 events.\n"
     ]
    }
   ],
   "source": [
    "# Cargamos el dataloader\n",
    "test_data = EventsData(data_dir='Mod_full_dist/train_data_fixed/', per=50);\n",
    "test_dataloader = DataLoader(test_data, batch_size=50, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf0961e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 0.0870932674407959 seconds per batch\n"
     ]
    }
   ],
   "source": [
    "\n",
    "times = np.zeros([n,1])\n",
    "\n",
    "for i in range(n):\n",
    "    start = time.time()\n",
    "    a,b=next(iter(test_dataloader))\n",
    "    stop = time.time()\n",
    "    times[i]=stop-start\n",
    "    #time.sleep(delay)\n",
    "    \n",
    "print(\"It took \"+str(np.sum(times)/n)+\" seconds per batch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db57d80",
   "metadata": {},
   "source": [
    "# New dataloader (not RAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a55b76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventsData(Dataset):\n",
    "    def __init__(self,data_dir,per=100,batch_size=500):\n",
    "        # Save the directory of the data\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "        # Get the names of the files\n",
    "        self.names=glob.glob(str(data_dir)+'*.hdf5')\n",
    "        # Number of files in the dir\n",
    "        self.size_dir=len(self.names)\n",
    "        \n",
    "        # Set the files acording to the percentage\n",
    "        self.size_dir=math.ceil(len(self.names)*(per/100))\n",
    "        self.names=self.names[0:self.size_dir]\n",
    "        #random.shuffle(self.names)\n",
    "        \n",
    "        # Get the number of events per file\n",
    "        print(self.names[1])\n",
    "        f = h5py.File(self.names[0],'r')\n",
    "        self.size_file=f['y'].shape[0]\n",
    "        \n",
    "        # Get the total number of events\n",
    "        self.total_events=0\n",
    "        for name in self.names:\n",
    "            f = h5py.File(name,'r')\n",
    "            y = f['y']\n",
    "            self.total_events+=y.shape[0]\n",
    "            \n",
    "        # Number of iterations to finish the dataset\n",
    "        self.batch_size=batch_size\n",
    "        self.iters=math.floor(self.total_events/batch_size)\n",
    "        self.iters_per_file= math.floor(self.size_file/batch_size)\n",
    "        self.real_events=self.batch_size*self.iters\n",
    "\n",
    "        print(\"There are \"+str(self.total_events)+\" events.\")\n",
    "        print(\"In \"+str(self.size_dir)+\" separate files.\")\n",
    "        print(\"Each file containing \"+str(self.size_file)+\" events.\")\n",
    "        print(\"In \"+str(self.iters)+\" iterations\")\n",
    "        print(\"The real number of events is: \"+str(self.real_events))\n",
    "        self.idx_file=-1\n",
    "\n",
    "\n",
    "    def get_len(self):\n",
    "        return self.real_events\n",
    "    \n",
    "    def get_iter(self):\n",
    "        # Returns the number of iteracions og getitem to finish the dataset\n",
    "        return self.iters\n",
    "\n",
    "    def get_batch(self, idx):\n",
    "        # Get the file that shall be opened\n",
    "        idx_file=math.floor(idx*self.batch_size/self.size_file)\n",
    "        if idx_file != self.idx_file:\n",
    "            self.f = h5py.File(self.names[idx_file],'r')\n",
    "            self.idx_file=idx_file\n",
    "            \n",
    "\n",
    "        ind=(idx*self.batch_size)%self.size_file\n",
    "  \n",
    "        # Get the events that will be extracted from the file\n",
    "        ind2=ind+self.batch_size\n",
    "        \n",
    "        data=torch.tensor(self.f['X1'][ind:ind2,:,:]).unsqueeze(dim=3)\n",
    "        \n",
    "        #Only for conv with modulus\n",
    "        data=data.transpose(1,3)\n",
    "        data=data.transpose(2,3)\n",
    "        \n",
    "        target=torch.zeros(self.batch_size,1)\n",
    "        \n",
    "        target[:,0]=torch.tensor(self.f['y'][ind:ind2,0])\n",
    "        target=torch.arccos(target)*(180.0/math.pi)\n",
    "    \n",
    "        \n",
    "        return data.float(),target.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3591e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset:\n",
      "Mod_full_dist/train_data_fixed/train_dataset_42.hdf5\n",
      "There are 431000 events.\n",
      "In 431 separate files.\n",
      "Each file containing 1000 events.\n",
      "In 8620 iterations\n",
      "The real number of events is: 431000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Test dataset:\")\n",
    "test=EventsData(data_dir='Mod_full_dist/train_data_fixed/', per=50, batch_size=50);\n",
    "print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5057eb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 0.0025989882151285807 seconds per batch\n"
     ]
    }
   ],
   "source": [
    "\n",
    "times = np.zeros([n,1])\n",
    "\n",
    "for i in range(n):\n",
    "    start = time.time()\n",
    "    a,b=test.get_batch(i)\n",
    "    stop = time.time()\n",
    "    times[i]=stop-start\n",
    "    #time.sleep(delay)\n",
    "    \n",
    "print(\"It took \"+str(np.sum(times)/n)+\" seconds per batch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0acd1c7",
   "metadata": {},
   "source": [
    "# RAM dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a209507",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventsData(Dataset):\n",
    "    def __init__(self,data_dir,per=100,batch_size=500):\n",
    "        # Save the directory of the data\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "        # Get the names of the files\n",
    "        self.names=glob.glob(str(data_dir)+'*.hdf5')\n",
    "        # Number of files in the dir\n",
    "        self.size_dir=len(self.names)\n",
    "        \n",
    "        # Set the files acording to the percentage\n",
    "        self.size_dir=math.ceil(len(self.names)*(per/100))\n",
    "        self.names=self.names[0:self.size_dir]\n",
    "        #random.shuffle(self.names)\n",
    "        \n",
    "        # Get the number of events per file\n",
    "        print(self.names[1])\n",
    "        f = h5py.File(self.names[0],'r')\n",
    "        self.size_file=f['y'].shape[0]\n",
    "        \n",
    "        # Get the total number of events\n",
    "        self.total_events=0\n",
    "        for name in self.names:\n",
    "            f = h5py.File(name,'r')\n",
    "            y = f['y']\n",
    "            self.total_events+=y.shape[0]\n",
    "            \n",
    "        # Load the whole dataset into the RAM\n",
    "        self.data_big = torch.zeros(self.total_events,25,161)\n",
    "        self.target_big = torch.zeros(self.total_events)\n",
    "\n",
    "        print(\"Reading \"+str(self.data_dir)+\" with \"+str(self.size_dir)+\" files.\")\n",
    "        for a in range(len(self.names)):\n",
    "            f = h5py.File(self.names[a],'r')\n",
    "            self.data_big[(a*1000):(((a+1)*1000))]=torch.tensor(f['X1'][:,:,:,0])\n",
    "            self.target_big[(a*1000):(((a+1)*1000))]=torch.tensor(f['y'][:,0])\n",
    "            self.target_big[(a*1000):(((a+1)*1000))].size()\n",
    "            \n",
    "        # Number of iterations to finish the dataset\n",
    "        self.batch_size=batch_size\n",
    "        self.iters=math.floor(self.total_events/batch_size)\n",
    "        self.iters_per_file= math.floor(self.size_file/batch_size)\n",
    "        self.real_events=self.batch_size*self.iters\n",
    "\n",
    "        print(\"There are \"+str(self.total_events)+\" events.\")\n",
    "        print(\"In \"+str(self.size_dir)+\" separate files.\")\n",
    "        print(\"Each file containing \"+str(self.size_file)+\" events.\")\n",
    "        print(\"In \"+str(self.iters)+\" iterations\")\n",
    "        print(\"The real number of events is: \"+str(self.real_events))\n",
    "        \n",
    "\n",
    "\n",
    "    def get_len(self):\n",
    "        return self.real_events\n",
    "    \n",
    "    def get_iter(self):\n",
    "        # Returns the number of iteracions og getitem to finish the dataset\n",
    "        return self.iters\n",
    "\n",
    "    def get_batch(self, idx):\n",
    "        # Get the file that shall be opened\n",
    "        ind1=idx*self.batch_size\n",
    "        ind2=((idx+1)*self.batch_size)\n",
    "        \n",
    "        #print(ind1)\n",
    "        #print(ind2)\n",
    "        \n",
    "        data=self.data_big[ind1:ind2,:,:]\n",
    "        target=self.target_big[ind1:ind2]\n",
    "        \n",
    "        # Get the events that will be extracted from the file\n",
    "        #ind2=ind+self.batch_size\n",
    "        \n",
    "        data=data.unsqueeze(dim=3)\n",
    "        target=target.unsqueeze(dim=1)\n",
    "        \n",
    "        #Only for conv with modulus\n",
    "        data=data.transpose(1,3)\n",
    "        data=data.transpose(2,3)\n",
    "        \n",
    "        \n",
    "        #target[:,0]=torch.tensor(f['y'][ind:ind2,0])\n",
    "        target=torch.arccos(target)*(180.0/math.pi)\n",
    "    \n",
    "        \n",
    "        return data.float(),target.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46e85a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset:\n",
      "Mod_full_dist/test_data_fixed/test_dataset_56.hdf5\n",
      "Reading Mod_full_dist/test_data_fixed/ with 60 files.\n",
      "There are 60000 events.\n",
      "In 60 separate files.\n",
      "Each file containing 1000 events.\n",
      "In 1200 iterations\n",
      "The real number of events is: 60000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Test dataset:\")\n",
    "test=EventsData(data_dir='Mod_full_dist/train_data_fixed/', per=50, batch_size=50);\n",
    "print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af33c968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 2.4814605712890625e-05 seconds per batch\n"
     ]
    }
   ],
   "source": [
    "\n",
    "times = np.zeros([n,1])\n",
    "\n",
    "for i in range(n):\n",
    "    start = time.time()\n",
    "    a,b=test.get_batch(n)\n",
    "    stop = time.time()\n",
    "    times[i]=stop-start\n",
    "    time.sleep(delay)\n",
    "    \n",
    "print(\"It took \"+str(np.sum(times)/n)+\" seconds per batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2c42e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
