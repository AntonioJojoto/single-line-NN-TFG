{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1343,
     "status": "ok",
     "timestamp": 1624614047300,
     "user": {
      "displayName": "antonio aslan suarez",
      "photoUrl": "",
      "userId": "04350384891875098633"
     },
     "user_tz": -120
    },
    "id": "yjjpgFssTJsy"
   },
   "outputs": [],
   "source": [
    "\n",
    "test_interval=5 \n",
    "\n",
    "# import libraries\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import shutil\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import glob\n",
    "import gzip\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import h5py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 536,
     "status": "ok",
     "timestamp": 1624614047834,
     "user": {
      "displayName": "antonio aslan suarez",
      "photoUrl": "",
      "userId": "04350384891875098633"
     },
     "user_tz": -120
    },
    "id": "hMzALyOVTMcp"
   },
   "outputs": [],
   "source": [
    "class EventsData(Dataset):\n",
    "    def __init__(self,data_dir,per=100,batch_size=500):\n",
    "        # Save the directory of the data\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "        # Get the names of the files\n",
    "        self.names=glob.glob(str(data_dir)+'*.hdf5')\n",
    "        # Number of files in the dir\n",
    "        self.size_dir=len(self.names)\n",
    "        \n",
    "        # Set the files acording to the percentage\n",
    "        self.size_dir=math.ceil(len(self.names)*(per/100))\n",
    "        self.names=self.names[0:self.size_dir]\n",
    "        # Sort the files alphabetically\n",
    "        self.names = sorted(self.names)\n",
    "        #random.shuffle(self.names)\n",
    "        \n",
    "        # Get the number of events per file\n",
    "        print(self.names[1])\n",
    "        f = h5py.File(self.names[0],'r')\n",
    "        self.size_file=f['y'].shape[0]\n",
    "        \n",
    "        # Get the total number of events\n",
    "        self.total_events=0\n",
    "        for name in self.names:\n",
    "            f = h5py.File(name,'r')\n",
    "            y = f['y']\n",
    "            self.total_events+=y.shape[0]\n",
    "\n",
    "        # Read the results from the Zenith and load them to RAM\n",
    "        zn = h5py.File('Z_results_zenith.hdf5','r')\n",
    "        # Identify if the dataloader is loading train, validation or test results\n",
    "        dataset_type = self.names[1][15]\n",
    "        print(dataset_type)\n",
    "        if dataset_type == 'r':\n",
    "          print(\"Train dataset type\")\n",
    "          self.z_zenith = torch.tensor([zn['Train_Z']])\n",
    "        if dataset_type == 'a':\n",
    "          print(\"Validation dataset type\")\n",
    "          self.z_zenith = torch.tensor([zn['Val_Z']])\n",
    "        if dataset_type == 'e':\n",
    "          print(\"test dataset type\")\n",
    "          self.z_zenith = torch.tensor([zn['Test_Z']])\n",
    "        \n",
    "        self.z_zenith=torch.squeeze(self.z_zenith)\n",
    "        print(self.z_zenith.size())\n",
    "        # Load the whole dataset into the RAM\n",
    "        self.data_big = torch.zeros(self.total_events,25,161)\n",
    "        self.target_big = torch.zeros(self.total_events)\n",
    "        \n",
    "\n",
    "        print(\"Reading \"+str(self.data_dir)+\" with \"+str(self.size_dir)+\" files.\")\n",
    "        for a in range(len(self.names)):\n",
    "            f = h5py.File(self.names[a],'r')\n",
    "            # The input of the network will the same for the Zenith model, distance, etc\n",
    "            self.data_big[(a*1000):(((a+1)*1000))]=torch.tensor(f['X1'][:,:,:,0])\n",
    "            # For distance in the target data, use the second component \n",
    "            # As the dimentions for the X2 tensor are:\n",
    "            # 0 -> Result from the BBFit\n",
    "            # 1 -> Distance \n",
    "            # 2 -> Zc\n",
    "            self.target_big[(a*1000):(((a+1)*1000))]=torch.tensor(f['X2'][:,1])\n",
    "            self.target_big[(a*1000):(((a+1)*1000))].size()\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        # Number of iterations to finish the dataset\n",
    "        self.batch_size=batch_size\n",
    "        self.iters=math.floor(self.total_events/batch_size)\n",
    "        self.iters_per_file= math.floor(self.size_file/batch_size)\n",
    "        self.real_events=self.batch_size*self.iters\n",
    "\n",
    "        print(\"There are \"+str(self.total_events)+\" events.\")\n",
    "        print(\"In \"+str(self.size_dir)+\" separate files.\")\n",
    "        print(\"Each file containing \"+str(self.size_file)+\" events.\")\n",
    "        print(\"In \"+str(self.iters)+\" iterations\")\n",
    "        print(\"The real number of events is: \"+str(self.real_events))\n",
    "        \n",
    "\n",
    "\n",
    "    def get_len(self):\n",
    "        return self.real_events\n",
    "    \n",
    "    def get_iter(self):\n",
    "        # Returns the number of iteracions og getitem to finish the dataset\n",
    "        return self.iters\n",
    "\n",
    "    def get_batch(self, idx):\n",
    "        # Get the file that shall be opened\n",
    "        ind1=idx*self.batch_size\n",
    "        ind2=((idx+1)*self.batch_size)\n",
    "        \n",
    "        \n",
    "        data=self.data_big[ind1:ind2,:,:]\n",
    "        target=self.target_big[ind1:ind2]\n",
    "        Z=self.z_zenith[ind1:ind2]\n",
    "        \n",
    "        # Get the events that will be extracted from the file\n",
    "        #ind2=ind+self.batch_size\n",
    "        \n",
    "        data=data.unsqueeze(dim=3)\n",
    "        target=target.unsqueeze(dim=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Only for conv with modulus\n",
    "        data=data.transpose(1,3)\n",
    "        data=data.transpose(2,3)    \n",
    "        \n",
    "        return data.float(),target.float(),Z.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1624614047835,
     "user": {
      "displayName": "antonio aslan suarez",
      "photoUrl": "",
      "userId": "04350384891875098633"
     },
     "user_tz": -120
    },
    "id": "nY59_IMwmWtC"
   },
   "outputs": [],
   "source": [
    "def init_data(percentage,batch):\n",
    "        # Save for exporting\n",
    "        percentage=percentage\n",
    "        batch_size=batch\n",
    "        # Initialize the datasets\n",
    "        print(\"Train dataset:\")\n",
    "        training_data = EventsData(data_dir='Mod_full_dist/train_data_fixed/', per=percentage, batch_size=batch);\n",
    "        print()\n",
    "\n",
    "        print(\"Validation dataset:\")\n",
    "        validation_data=EventsData(data_dir='Mod_full_dist/validation_data_fixed/', per=percentage, batch_size=batch);\n",
    "        print()\n",
    "\n",
    "        print(\"Test dataset:\")\n",
    "        test_data=EventsData(data_dir='Mod_full_dist/test_data_fixed/', per=percentage, batch_size=batch);\n",
    "        print()\n",
    "\n",
    "        return training_data,validation_data,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 417125,
     "status": "ok",
     "timestamp": 1624614464955,
     "user": {
      "displayName": "antonio aslan suarez",
      "photoUrl": "",
      "userId": "04350384891875098633"
     },
     "user_tz": -120
    },
    "id": "-Rdkj7yWm6nz",
    "outputId": "f5779884-8547-4141-b87e-11e01022a742"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset:\n",
      "Mod_full_dist/train_data_fixed/train_dataset_1.hdf5\n",
      "r\n",
      "Train dataset type\n",
      "torch.Size([862000, 2])\n",
      "Reading Mod_full_dist/train_data_fixed/ with 862 files.\n",
      "There are 862000 events.\n",
      "In 862 separate files.\n",
      "Each file containing 1000 events.\n",
      "In 862 iterations\n",
      "The real number of events is: 862000\n",
      "\n",
      "Validation dataset:\n",
      "Mod_full_dist/validation_data_fixed/validation_dataset_1.hdf5\n",
      "a\n",
      "Validation dataset type\n",
      "torch.Size([247000, 2])\n",
      "Reading Mod_full_dist/validation_data_fixed/ with 247 files.\n",
      "There are 247000 events.\n",
      "In 247 separate files.\n",
      "Each file containing 1000 events.\n",
      "In 247 iterations\n",
      "The real number of events is: 247000\n",
      "\n",
      "Test dataset:\n",
      "Mod_full_dist/test_data_fixed/test_dataset_1.hdf5\n",
      "e\n",
      "test dataset type\n",
      "torch.Size([120000, 2])\n",
      "Reading Mod_full_dist/test_data_fixed/ with 120 files.\n",
      "There are 120000 events.\n",
      "In 120 separate files.\n",
      "Each file containing 1000 events.\n",
      "In 120 iterations\n",
      "The real number of events is: 120000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size_used = 1000 # Has to be fixed\n",
    "percentage_used = 100\n",
    "\n",
    "train,val,test = init_data(percentage_used,batch_size_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1624614464956,
     "user": {
      "displayName": "antonio aslan suarez",
      "photoUrl": "",
      "userId": "04350384891875098633"
     },
     "user_tz": -120
    },
    "id": "im6PmYOgR0Q6"
   },
   "outputs": [],
   "source": [
    "# Nueva funciÃ³n de coste\n",
    "def custom_loss(target,mu,sigma):\n",
    "    # Create the normal distribution\n",
    "    #print(mu[1])\n",
    "    dist = torch.distributions.Normal(loc=mu, scale=sigma)\n",
    "    # Obtain the -PDF and reduce it to the mean\n",
    "    # Shall return a real number only\n",
    "    loss = torch.mean(-dist.log_prob(target))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 1023,
     "status": "ok",
     "timestamp": 1624614465975,
     "user": {
      "displayName": "antonio aslan suarez",
      "photoUrl": "",
      "userId": "04350384891875098633"
     },
     "user_tz": -120
    },
    "id": "0uNQbYDjTQ3B"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def weight_reset(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()\n",
    "\n",
    "# This class will contain a NN model and all of its functions and data \n",
    "# It won't be pushed to the GPU\n",
    "class Net_Info():\n",
    "    # Constructor for the predictor\n",
    "    # Data regarding the NN shall be passed here\n",
    "    def __init__(self,model,folder,name):\n",
    "        # Route to save the model\n",
    "        self.folder=folder\n",
    "        self.name=name\n",
    "\n",
    "        # Set the optimizer and loss functions\n",
    "        self.optimizer=optim.SGD(model.parameters(),lr=0.002)\n",
    "        #self.loss_function=nn.L1Loss()\n",
    "        \n",
    "    # Function to set the model name and folder\n",
    "    # Used to save the name in it\n",
    "    def location(self,name,folder):\n",
    "        self.folder=folder\n",
    "        self.model_name=name\n",
    "\n",
    "    def init_data(self,percentage,batch):\n",
    "        # Save for exporting\n",
    "        self.percentage=percentage\n",
    "        self.batch_size=batch\n",
    "        \n",
    "\n",
    "    # Save all the data from the class, except the arquitecture of the net\n",
    "    def save_params(self,model):\n",
    "        # Open the text file\n",
    "        file1=open(str(self.folder)+\"/\"+str(self.name)+\"_params.txt\",\"w+\")\n",
    "        file1.write(\"Data Info:\\n\")\n",
    "        file1.write(str(self.percentage)+\"\\n\"+str(self.batch_size)+\"\\n\"+str(self.n_epochs)+\"\\n\")\n",
    "        file1.write(\"Last Val Info:\\n\")\n",
    "        file1.write(str(self.valid_loss_min)+\"\\n\")\n",
    "        file1.write(\"Last Errors:\\n\")\n",
    "        file1.write(str(self.MAE)+\"\\n\"+ str(self.RMSE)+\"\\n\")\n",
    "        file1.write(\"Loss and Optimier\\n\")\n",
    "        #file1.write(str(self.loss_function)+\"\\n\")\n",
    "        file1.write(str(self.optimizer))\n",
    "        file1.close()\n",
    "        file2=open(str(self.folder)+\"/\"+str(self.name)+\"_arquitecture.txt\",\"w+\")\n",
    "        file2.write(str(model))\n",
    "        file2.close()\n",
    "\n",
    "    def plot_loss(self):\n",
    "        plt.figure(figsize=(10,10),dpi=800)\n",
    "        plt.plot(self.train_loss_temp[:self.n_epochs],linewidth=3.0,color=\"r\")\n",
    "        plt.plot(self.valid_loss_temp[:self.n_epochs],linewidth=3.0,color=\"g\")\n",
    "        plt.title('{} | MAE = {:.3f} | RMSE = {:.3f}'.format(self.name,self.MAE,self.RMSE))\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.legend(['Training Loss','Validation Loss'], loc = 'upper left')\n",
    "        # plt.show()\n",
    "        plt.savefig(str(self.folder)+\"/\"+str(self.name)+\"_loss.jpg\")\n",
    "\n",
    "    def plot_pred(self):\n",
    "        plt.figure(figsize=(5,5),dpi=100)\n",
    "        plt.scatter(self.target_temp,self.output_temp)\n",
    "        plt.xlabel('True Values ')\n",
    "        plt.ylabel('Predictions ')\n",
    "        plt.axis('equal')\n",
    "        plt.axis('square')\n",
    "        plt.title(\"Target vs. Prediction values of \"+str(self.name))\n",
    "        plt.savefig(str(self.folder)+\"/\"+str(self.name)+\"_scatter.jpg\")\n",
    "        plt.close();\n",
    "      \n",
    "        \n",
    "\n",
    "    def train_model(self,net,n_epochs,test_inter,train,val,test,valid_loss_min=np.Inf):\n",
    "        self.n_epochs = n_epochs\n",
    "        # If valid loss is not inf, the load the model\n",
    "        if valid_loss_min>1000:\n",
    "            self.valid_loss_min = np.Inf # track change in validation loss\n",
    "        else:\n",
    "            self.valid_loss_min = valid_loss_min\n",
    "            net.load_state_dict(torch.load(str(self.folder)+'/'+str(self.name)+'.pt'))\n",
    "            \n",
    "\n",
    "        self.train_loss_temp = np.zeros([n_epochs,1])\n",
    "        self.valid_loss_temp = np.zeros([n_epochs,1])\n",
    "        # If this number goes to zero, the training will stop\n",
    "        last_save=10\n",
    "        for epoch in range(1, n_epochs+1):\n",
    "            # keep track of training and validation loss\n",
    "            train_loss = 0.0\n",
    "            valid_loss = 0.0\n",
    "\n",
    "\n",
    "\n",
    "            # train the net #\n",
    "            net.train()\n",
    "            for batch in range(train.get_iter()):\n",
    "                # Get the data\n",
    "                data,target,Z=train.get_batch(batch)\n",
    "                # move tensors to GPU if CUDA is available\n",
    "                data, target, Z = data.cuda(), target.cuda(), Z.cuda()\n",
    "                # clear the gradients of all optimized variables\n",
    "                self.optimizer.zero_grad()\n",
    "                # forward pass: compute predicted outputs by passing inputs to the net\n",
    "                mu,sigma = net(data,Z)\n",
    "                # calculate the batch loss\n",
    "                loss = custom_loss(target,mu,sigma)\n",
    "                # backward pass: compute gradient of the loss with respect to net parameters\n",
    "                loss.backward()\n",
    "                # perform a single optimization step (parameter update)\n",
    "                self.optimizer.step()\n",
    "                # update training loss\n",
    "                # and reboot if nan\n",
    "                if math.isnan(loss.item()) == True:\n",
    "                    net.apply(weight_reset)\n",
    "                    print(\"Reseteo insacioso\")\n",
    "                train_loss += loss.item()\n",
    "\n",
    "\n",
    "            # validate the net #\n",
    "            net.eval()\n",
    "            for batch in range(val.get_iter()):\n",
    "                # Get the data\n",
    "                data,target,Z=train.get_batch(batch)\n",
    "                # move tensors to GPU if CUDA is available\n",
    "                data, target, Z = data.cuda(), target.cuda(), Z.cuda()\n",
    "                # forward pass: compute predicted outputs by passing inputs to the net\n",
    "                mu,sigma = net(data,Z)\n",
    "                # calculate the batch loss\n",
    "                loss = custom_loss(target,mu,sigma)\n",
    "                # update average validation loss \n",
    "                valid_loss += loss.item()\n",
    "            #el data,target\n",
    "            # calculate average losses\n",
    "            train_loss = train_loss/train.get_len()\n",
    "            valid_loss = valid_loss/val.get_len()\n",
    "\n",
    "            # Append the losses to the historical ones\n",
    "            self.train_loss_temp[epoch-1]=train_loss\n",
    "            self.valid_loss_temp[epoch-1]=valid_loss            \n",
    "\n",
    "            # print training/validation statistics \n",
    "            print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "                epoch, train_loss, valid_loss))\n",
    "            # Save the net if the loss goes down\n",
    "            # Show also the % of error down\n",
    "            if valid_loss <= self.valid_loss_min:\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving net ...'.format(\n",
    "                self.valid_loss_min,\n",
    "                valid_loss))\n",
    "                torch.save(net.state_dict(), str(self.folder)+'/'+str(self.name)+'.pt')\n",
    "                # Show by how much the validation loss has dropped\n",
    "                valid_percent=((self.valid_loss_min-valid_loss)/self.valid_loss_min)*100\n",
    "                print('Validation loss has dropped {:.2f}%'.format(valid_percent))\n",
    "                self.valid_loss_min = valid_loss\n",
    "                if valid_percent>=1:\n",
    "                    last_save=10\n",
    "                    print(\"Keep training\")\n",
    "                else:\n",
    "                    last_save += 2\n",
    "            else:\n",
    "                last_save -= 1;\n",
    "                \n",
    "            # Show iterations to stop\n",
    "            print('Training will stop in '+str(last_save)+\" epochs ...\")\n",
    "            print(\"\")\n",
    "            \n",
    "            # Check the test error and the \n",
    "            if (epoch%test_interval)==0:\n",
    "                # self.target_temp=np.zeros([self.test_data.get_len(),1])\n",
    "                # self.output_temp=np.zeros([self.test_data.get_len(),1])\n",
    "                # Init the errors\n",
    "                MAE=0\n",
    "                MSE=0\n",
    "                for batch in range(test.get_iter()):\n",
    "                    # Get the data\n",
    "                    data,target,Z=test.get_batch(batch)\n",
    "                    # move tensors to GPU if CUDA is available\n",
    "                    data,Z = data.cuda(),Z.cuda()\n",
    "                    # Get the results from the foward pass to the CPU \n",
    "                    # And get it as an numpy matrix\n",
    "                    mu,sigma = net(data,Z)#.cpu().detach().numpy()\n",
    "                    target=target.numpy()\n",
    "                    output = mu.cpu().detach().numpy()\n",
    "                    # calculate the batch loss\n",
    "                    MAE += np.sum(np.abs(output-target))\n",
    "                    MSE += np.sum((output-target)**2)\n",
    "                    # print(output)\n",
    "                    # Append to the historical value\n",
    "                    # self.target_temp[(batch*self.batch_size):((batch+1)*self.batch_size)]=target\n",
    "                    # self.output_temp[(batch*self.batch_size):((batch+1)*self.batch_size)]=output\n",
    "\n",
    "                # self.plot_pred()\n",
    "                # calculate average losses\n",
    "                self.MAE = MAE/test.get_len()\n",
    "                self.RMSE = np.sqrt(MSE/test.get_len())\n",
    "                    \n",
    "                print(\"The MAE is \"+str(self.MAE))\n",
    "                print(\"The RMSE is \"+str(self.RMSE))\n",
    "\n",
    "            # Check if the net is not progressing\n",
    "            if last_save<=0:\n",
    "                print(\"Early stopping\")\n",
    "                self.n_epochs=epoch\n",
    "                break\n",
    "        # Compute the test error for the best performing model\n",
    "        # Load the model\n",
    "        net.load_state_dict(torch.load(str(self.folder)+'/'+str(self.name)+'.pt'))\n",
    "        MAE=0\n",
    "        MSE=0\n",
    "        for batch in range(test.get_iter()):\n",
    "            # Get the data\n",
    "            data,target,Z=test.get_batch(batch)\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            data,Z = data.cuda(),Z.cuda()\n",
    "            # Get the results from the foward pass to the CPU \n",
    "            # And get it as an numpy matrix\n",
    "            mu,sigma = net(data,Z)\n",
    "            target=target.numpy()\n",
    "            output = mu.cpu().detach().numpy()\n",
    "            # calculate the batch loss\n",
    "            MAE += np.sum(np.abs(output-target))\n",
    "            MSE += np.sum((output-target)**2)\n",
    "            # print(output)\n",
    "            # Append to the historical value\n",
    "            # self.target_temp[(batch*self.batch_size):((batch+1)*self.batch_size)]=target\n",
    "            # self.output_temp[(batch*self.batch_size):((batch+1)*self.batch_size)]=output\n",
    "\n",
    "        # self.plot_pred()\n",
    "        # calculate average losses\n",
    "        self.MAE = MAE/test.get_len()\n",
    "        self.RMSE = np.sqrt(MSE/test.get_len())\n",
    "\n",
    "        print(\"The MAE is \"+str(self.MAE))\n",
    "        print(\"The RMSE is \"+str(self.RMSE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 420,
     "status": "ok",
     "timestamp": 1624614751914,
     "user": {
      "displayName": "antonio aslan suarez",
      "photoUrl": "",
      "userId": "04350384891875098633"
     },
     "user_tz": -120
    },
    "id": "bQZd5ThNSLCK"
   },
   "outputs": [],
   "source": [
    "# This class will contain the NN arquitecture, it will be pushed to the GPU\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        kernel_size=(3,3)\n",
    "        \n",
    "        #conv layer, sees 25x161x1 tensor\n",
    "        self.conv1 = nn.Conv2d(1, 3, kernel_size,padding=1)\n",
    "        self.conv2 = nn.Conv2d(3, 12, kernel_size,padding=1)\n",
    "        #self.conv3 = nn.Conv2d(32, 64, kernel_size,padding=1)\n",
    "        #self.conv4 = nn.Conv2d(64, 128, kernel_size,padding=1)\n",
    "\n",
    "        \n",
    "        self.pool = nn.MaxPool2d((2, 2))\n",
    "        # Two more inputs for the mean and std of the Z component\n",
    "        input_flatten = 2880 + 2\n",
    "        hidden_1 = 600\n",
    "        hidden_2 = 200\n",
    "        hidden_3 = 50\n",
    "        # linear layer (784 -> hidden_1)\n",
    "        self.fc1 = nn.Linear(input_flatten, hidden_1)\n",
    "        # linear layer (n_hidden -> hidden_2)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden_2, hidden_3)\n",
    "        self.mu = nn.Linear(hidden_3, 1)\n",
    "        self.sigma = nn.Linear(hidden_3,1)\n",
    "        \n",
    "        # dropout layer (p=0.2)\n",
    "        # dropout prevents overfitting of data\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "\n",
    "    def forward(self, x,z):\n",
    "        input_flatten = 2880\n",
    "        # add sequence of convolutional and max pooling layers\n",
    "\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        #x = self.pool(F.relu(self.conv3(x)))\n",
    "        #x = self.pool(F.relu(self.conv4(x)))\n",
    "   \n",
    "        x = x.reshape(-1, input_flatten)\n",
    "        x = torch.cat((x,z),dim=1)\n",
    "\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        mu = self.mu(x)\n",
    "        sigma = torch.exp(self.sigma(x))\n",
    "        return mu,sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1280364,
     "status": "error",
     "timestamp": 1624616041982,
     "user": {
      "displayName": "antonio aslan suarez",
      "photoUrl": "",
      "userId": "04350384891875098633"
     },
     "user_tz": -120
    },
    "id": "Hg9WmPRKTbqi",
    "outputId": "43090c95-a3f4-4097-fbcf-214e55b60288"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance_MDN directory already exists\n",
      "Using decive cuda\n",
      "Running Dist_MDNH_One_exp1_0\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=2882, out_features=600, bias=True)\n",
      "  (fc2): Linear(in_features=600, out_features=200, bias=True)\n",
      "  (fc3): Linear(in_features=200, out_features=50, bias=True)\n",
      "  (mu): Linear(in_features=50, out_features=1, bias=True)\n",
      "  (sigma): Linear(in_features=50, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      ")\n",
      "Epoch: 1 \tTraining Loss: 0.004233 \tValidation Loss: 0.003510\n",
      "Validation loss decreased (inf --> 0.003510).  Saving net ...\n",
      "Validation loss has dropped nan%\n",
      "Training will stop in 12 epochs ...\n",
      "\n",
      "Epoch: 2 \tTraining Loss: 0.003595 \tValidation Loss: 0.003431\n",
      "Validation loss decreased (0.003510 --> 0.003431).  Saving net ...\n",
      "Validation loss has dropped 2.24%\n",
      "Keep training\n",
      "Training will stop in 10 epochs ...\n",
      "\n",
      "Epoch: 3 \tTraining Loss: 0.003528 \tValidation Loss: 0.003385\n",
      "Validation loss decreased (0.003431 --> 0.003385).  Saving net ...\n",
      "Validation loss has dropped 1.35%\n",
      "Keep training\n",
      "Training will stop in 10 epochs ...\n",
      "\n",
      "Epoch: 4 \tTraining Loss: 0.003473 \tValidation Loss: 0.003377\n",
      "Validation loss decreased (0.003385 --> 0.003377).  Saving net ...\n",
      "Validation loss has dropped 0.22%\n",
      "Training will stop in 12 epochs ...\n",
      "\n",
      "Epoch: 5 \tTraining Loss: 0.003431 \tValidation Loss: 0.003301\n",
      "Validation loss decreased (0.003377 --> 0.003301).  Saving net ...\n",
      "Validation loss has dropped 2.26%\n",
      "Keep training\n",
      "Training will stop in 10 epochs ...\n",
      "\n",
      "The MAE is 5.310551253255208\n",
      "The RMSE is 8.081735550716736\n",
      "Epoch: 6 \tTraining Loss: 0.003387 \tValidation Loss: 0.003266\n",
      "Validation loss decreased (0.003301 --> 0.003266).  Saving net ...\n",
      "Validation loss has dropped 1.06%\n",
      "Keep training\n",
      "Training will stop in 10 epochs ...\n",
      "\n",
      "Epoch: 7 \tTraining Loss: 0.003345 \tValidation Loss: 0.003234\n",
      "Validation loss decreased (0.003266 --> 0.003234).  Saving net ...\n",
      "Validation loss has dropped 0.96%\n",
      "Training will stop in 12 epochs ...\n",
      "\n",
      "Epoch: 8 \tTraining Loss: 0.003314 \tValidation Loss: 0.003212\n",
      "Validation loss decreased (0.003234 --> 0.003212).  Saving net ...\n",
      "Validation loss has dropped 0.68%\n",
      "Training will stop in 14 epochs ...\n",
      "\n",
      "Epoch: 9 \tTraining Loss: 0.003279 \tValidation Loss: 0.003182\n",
      "Validation loss decreased (0.003212 --> 0.003182).  Saving net ...\n",
      "Validation loss has dropped 0.94%\n",
      "Training will stop in 16 epochs ...\n",
      "\n",
      "Epoch: 10 \tTraining Loss: 0.003249 \tValidation Loss: 0.003162\n",
      "Validation loss decreased (0.003182 --> 0.003162).  Saving net ...\n",
      "Validation loss has dropped 0.63%\n",
      "Training will stop in 18 epochs ...\n",
      "\n",
      "The MAE is 4.881573616536459\n",
      "The RMSE is 7.657615120645429\n",
      "Epoch: 11 \tTraining Loss: 0.003225 \tValidation Loss: 0.003160\n",
      "Validation loss decreased (0.003162 --> 0.003160).  Saving net ...\n",
      "Validation loss has dropped 0.08%\n",
      "Training will stop in 20 epochs ...\n",
      "\n",
      "Epoch: 12 \tTraining Loss: 0.003264 \tValidation Loss: 0.003129\n",
      "Validation loss decreased (0.003160 --> 0.003129).  Saving net ...\n",
      "Validation loss has dropped 0.96%\n",
      "Training will stop in 22 epochs ...\n",
      "\n",
      "Epoch: 13 \tTraining Loss: 0.857046 \tValidation Loss: 0.003122\n",
      "Validation loss decreased (0.003129 --> 0.003122).  Saving net ...\n",
      "Validation loss has dropped 0.22%\n",
      "Training will stop in 24 epochs ...\n",
      "\n",
      "Epoch: 14 \tTraining Loss: 0.034348 \tValidation Loss: 0.003107\n",
      "Validation loss decreased (0.003122 --> 0.003107).  Saving net ...\n",
      "Validation loss has dropped 0.47%\n",
      "Training will stop in 26 epochs ...\n",
      "\n",
      "Epoch: 15 \tTraining Loss: 0.003259 \tValidation Loss: 0.003101\n",
      "Validation loss decreased (0.003107 --> 0.003101).  Saving net ...\n",
      "Validation loss has dropped 0.22%\n",
      "Training will stop in 28 epochs ...\n",
      "\n",
      "The MAE is 4.778096659342448\n",
      "The RMSE is 7.509751910883663\n",
      "Epoch: 16 \tTraining Loss: 82489.109578 \tValidation Loss: 0.003098\n",
      "Validation loss decreased (0.003101 --> 0.003098).  Saving net ...\n",
      "Validation loss has dropped 0.08%\n",
      "Training will stop in 30 epochs ...\n",
      "\n",
      "Epoch: 17 \tTraining Loss: 0.782978 \tValidation Loss: 0.003084\n",
      "Validation loss decreased (0.003098 --> 0.003084).  Saving net ...\n",
      "Validation loss has dropped 0.45%\n",
      "Training will stop in 32 epochs ...\n",
      "\n",
      "Epoch: 18 \tTraining Loss: 0.003153 \tValidation Loss: 0.003084\n",
      "Validation loss decreased (0.003084 --> 0.003084).  Saving net ...\n",
      "Validation loss has dropped 0.01%\n",
      "Training will stop in 34 epochs ...\n",
      "\n",
      "Epoch: 19 \tTraining Loss: 31.532642 \tValidation Loss: 0.003073\n",
      "Validation loss decreased (0.003084 --> 0.003073).  Saving net ...\n",
      "Validation loss has dropped 0.35%\n",
      "Training will stop in 36 epochs ...\n",
      "\n",
      "Epoch: 20 \tTraining Loss: 0.003329 \tValidation Loss: 0.003074\n",
      "Training will stop in 35 epochs ...\n",
      "\n",
      "The MAE is 4.769337219238281\n",
      "The RMSE is 7.4463870530096115\n",
      "Epoch: 21 \tTraining Loss: 40.242352 \tValidation Loss: 0.003071\n",
      "Validation loss decreased (0.003073 --> 0.003071).  Saving net ...\n",
      "Validation loss has dropped 0.06%\n",
      "Training will stop in 37 epochs ...\n",
      "\n",
      "Epoch: 22 \tTraining Loss: 2.323433 \tValidation Loss: 0.003063\n",
      "Validation loss decreased (0.003071 --> 0.003063).  Saving net ...\n",
      "Validation loss has dropped 0.25%\n",
      "Training will stop in 39 epochs ...\n",
      "\n",
      "Epoch: 23 \tTraining Loss: 124.403367 \tValidation Loss: 0.003062\n",
      "Validation loss decreased (0.003063 --> 0.003062).  Saving net ...\n",
      "Validation loss has dropped 0.04%\n",
      "Training will stop in 41 epochs ...\n",
      "\n",
      "Epoch: 24 \tTraining Loss: 0.003147 \tValidation Loss: 0.003061\n",
      "Validation loss decreased (0.003062 --> 0.003061).  Saving net ...\n",
      "Validation loss has dropped 0.04%\n",
      "Training will stop in 43 epochs ...\n",
      "\n",
      "Epoch: 25 \tTraining Loss: 0.011123 \tValidation Loss: 0.003053\n",
      "Validation loss decreased (0.003061 --> 0.003053).  Saving net ...\n",
      "Validation loss has dropped 0.28%\n",
      "Training will stop in 45 epochs ...\n",
      "\n",
      "The MAE is 4.653408467610677\n",
      "The RMSE is 7.408392517105178\n",
      "Epoch: 26 \tTraining Loss: 0.003392 \tValidation Loss: 0.003052\n",
      "Validation loss decreased (0.003053 --> 0.003052).  Saving net ...\n",
      "Validation loss has dropped 0.03%\n",
      "Training will stop in 47 epochs ...\n",
      "\n",
      "Epoch: 27 \tTraining Loss: 0.003117 \tValidation Loss: 0.003228\n",
      "Training will stop in 46 epochs ...\n",
      "\n",
      "Epoch: 28 \tTraining Loss: 0.003117 \tValidation Loss: 0.003050\n",
      "Validation loss decreased (0.003052 --> 0.003050).  Saving net ...\n",
      "Validation loss has dropped 0.07%\n",
      "Training will stop in 48 epochs ...\n",
      "\n",
      "Epoch: 29 \tTraining Loss: 0.003112 \tValidation Loss: 0.003135\n",
      "Training will stop in 47 epochs ...\n",
      "\n",
      "Epoch: 30 \tTraining Loss: 0.003113 \tValidation Loss: 0.003053\n",
      "Training will stop in 46 epochs ...\n",
      "\n",
      "The MAE is 4.632329130045573\n",
      "The RMSE is 7.380681555341225\n",
      "Epoch: 31 \tTraining Loss: 0.003117 \tValidation Loss: 0.003152\n",
      "Training will stop in 45 epochs ...\n",
      "\n",
      "Epoch: 32 \tTraining Loss: 0.003115 \tValidation Loss: 0.004222\n",
      "Training will stop in 44 epochs ...\n",
      "\n",
      "Epoch: 33 \tTraining Loss: 0.003304 \tValidation Loss: 1.535589\n",
      "Training will stop in 43 epochs ...\n",
      "\n",
      "Epoch: 34 \tTraining Loss: 0.003107 \tValidation Loss: 0.003045\n",
      "Validation loss decreased (0.003050 --> 0.003045).  Saving net ...\n",
      "Validation loss has dropped 0.16%\n",
      "Training will stop in 45 epochs ...\n",
      "\n",
      "Epoch: 35 \tTraining Loss: 0.003156 \tValidation Loss: 0.003033\n",
      "Validation loss decreased (0.003045 --> 0.003033).  Saving net ...\n",
      "Validation loss has dropped 0.37%\n",
      "Training will stop in 47 epochs ...\n",
      "\n",
      "The MAE is 4.615387467447917\n",
      "The RMSE is 7.324809528951362\n",
      "Epoch: 36 \tTraining Loss: 0.003104 \tValidation Loss: 0.003035\n",
      "Training will stop in 46 epochs ...\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-c89c03a2a77e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpercentage_used\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size_used\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# This next may be inside of a function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_interval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0;31m# Save the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-cfd713edcda5>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, net, n_epochs, test_inter, train, val, test, valid_loss_min)\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;31m# calculate the batch loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                 \u001b[0;31m# backward pass: compute gradient of the loss with respect to net parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-25c5a5a0ffd3>\u001b[0m in \u001b[0;36mcustom_loss\u001b[0;34m(target, mu, sigma)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Create the normal distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#print(mu[1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Obtain the -PDF and reduce it to the mean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Shall return a real number only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/distributions/normal.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mbatch_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNormal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     51\u001b[0m                     \u001b[0;32mcontinue\u001b[0m  \u001b[0;31m# skip checking lazily-constructed args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconstraint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The parameter {} has invalid values\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDistribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The parameter loc has invalid values"
     ]
    }
   ],
   "source": [
    "# Get the arguments from the command promt\n",
    "folder_name = \"Distance_MDN\"\n",
    "net_name = \"Dist_MDNH_One_exp1\"\n",
    "# 0.01 muy lento\n",
    "lr = [1.0]\n",
    "n_epochs =  150 # Has to be fixed\n",
    "n_nets = 1\n",
    "\n",
    "# Try to create the folder name\n",
    "try:\n",
    "    os.mkdir(str(folder_name))\n",
    "except FileExistsError:\n",
    "    print(str(folder_name)+ \" directory already exists\")\n",
    "\n",
    "# Copy the modified python script with the net name into the subfolder\n",
    "shutil.copy2(sys.argv[0],str(folder_name)+\"/\"+str(net_name)+\".py\")  \n",
    "\n",
    "# Check if cuda is available and set as default device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using decive \"+str(device))\n",
    "# Run all the models\n",
    "# Run all the learning rates\n",
    "\n",
    "for b in range(n_nets):\n",
    "    # initialize the NN\n",
    "    new_name=str(net_name)+\"_\"+str(b)\n",
    "    print(\"Running \"+str(new_name))\n",
    "    net=Net()\n",
    "    print(net)\n",
    "    net.cuda()\n",
    "    # Initialize the Net_Info object\n",
    "    Model=Net_Info(net,folder_name,new_name)\n",
    "    Model.optimizer=optim.Adadelta(net.parameters(),lr=0.1)\n",
    "    # Initialize the datasets\n",
    "    Model.init_data(percentage_used,batch_size_used)\n",
    "    # This next may be inside of a function\n",
    "    Model.train_model(net,n_epochs,test_interval,train,val,test)\n",
    "    # Save the model\n",
    "    Model.save_params(net)\n",
    "    Model.plot_loss() \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO+IF3CvQJmhl5sFOW1xkj8",
   "machine_shape": "hm",
   "mount_file_id": "1xmw1Ct4kebgyCNZJBKz34Ov4Ju1k_tV-",
   "name": "Distance_MDNH_exp.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
