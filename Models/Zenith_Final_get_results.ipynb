{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Conv_7_MDNH_exp_get_results.ipynb","provenance":[],"machine_shape":"hm","mount_file_id":"1qotnUG-Fhix0HwjyW1mfsGWknH04O-5U","authorship_tag":"ABX9TyNwbnqKuRUtFXGbaYaykkiw"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"BWSzO7bVfWxM"},"source":["## Get the input data and net data"]},{"cell_type":"code","metadata":{"id":"J-iZ9pX3T2FH"},"source":["!cp drive/MyDrive/NN_Energy/Dist_for_Collab.tar.gz .\n","!tar -zxvf Dist_for_Collab.tar.gz"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TdU_wF5DTx8D","executionInfo":{"status":"ok","timestamp":1624479411998,"user_tz":-120,"elapsed":315,"user":{"displayName":"antonio aslan suarez","photoUrl":"","userId":"04350384891875098633"}}},"source":["!cp -r drive/MyDrive/NN_Energy/Conv7_MDN/ ."],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"yjjpgFssTJsy","executionInfo":{"status":"ok","timestamp":1624481349106,"user_tz":-120,"elapsed":1490,"user":{"displayName":"antonio aslan suarez","photoUrl":"","userId":"04350384891875098633"}}},"source":["\n","test_interval=5 \n","\n","# import libraries\n","import torch\n","from torch.utils.data import Dataset\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import shutil\n","import re\n","import math\n","import time\n","import glob\n","import gzip\n","import sys\n","import matplotlib.pyplot as plt\n","import os\n","import random\n","import h5py\n"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"hMzALyOVTMcp","executionInfo":{"status":"ok","timestamp":1624481351142,"user_tz":-120,"elapsed":443,"user":{"displayName":"antonio aslan suarez","photoUrl":"","userId":"04350384891875098633"}}},"source":["class EventsData(Dataset):\n","    def __init__(self,data_dir,per=100,batch_size=500):\n","        # Save the directory of the data\n","        self.data_dir = data_dir\n","        \n","        # Get the names of the files\n","        self.names=glob.glob(str(data_dir)+'*.hdf5')\n","        # Number of files in the dir\n","        self.size_dir=len(self.names)\n","        \n","        # Set the files acording to the percentage\n","        self.size_dir=math.ceil(len(self.names)*(per/100))\n","        self.names=self.names[0:self.size_dir]\n","        # Sort the files alphabetically\n","        self.names = sorted(self.names)\n","        #random.shuffle(self.names)\n","        \n","        # Get the number of events per file\n","        print(self.names[0])\n","        f = h5py.File(self.names[0],'r')\n","        self.size_file=f['y'].shape[0]\n","        \n","        # Get the total number of events\n","        self.total_events=0\n","        for name in self.names:\n","            f = h5py.File(name,'r')\n","            y = f['y']\n","            self.total_events+=y.shape[0]\n","            \n","        # Load the whole dataset into the RAM\n","        self.data_big = torch.zeros(self.total_events,25,161)\n","        self.target_big = torch.zeros(self.total_events)\n","\n","        print(\"Reading \"+str(self.data_dir)+\" with \"+str(self.size_dir)+\" files.\")\n","        for a in range(len(self.names)):\n","            f = h5py.File(self.names[a],'r')\n","            self.data_big[(a*1000):(((a+1)*1000))]=torch.tensor(f['X1'][:,:,:,0])\n","            self.target_big[(a*1000):(((a+1)*1000))]=torch.tensor(f['y'][:,0])\n","            self.target_big[(a*1000):(((a+1)*1000))].size()\n","            \n","        # Number of iterations to finish the dataset\n","        self.batch_size=batch_size\n","        self.iters=math.floor(self.total_events/batch_size)\n","        self.iters_per_file= math.floor(self.size_file/batch_size)\n","        self.real_events=self.batch_size*self.iters\n","\n","        print(\"There are \"+str(self.total_events)+\" events.\")\n","        print(\"In \"+str(self.size_dir)+\" separate files.\")\n","        print(\"Each file containing \"+str(self.size_file)+\" events.\")\n","        print(\"In \"+str(self.iters)+\" iterations\")\n","        print(\"The real number of events is: \"+str(self.real_events))\n","        \n","\n","\n","    def get_len(self):\n","        return self.real_events\n","    \n","    def get_iter(self):\n","        # Returns the number of iteracions og getitem to finish the dataset\n","        return self.iters\n","\n","    def get_batch(self, idx):\n","        # Get the file that shall be opened\n","        ind1=idx*self.batch_size\n","        ind2=((idx+1)*self.batch_size)\n","        \n","        #print(ind1)\n","        #print(ind2)\n","        \n","        data=self.data_big[ind1:ind2,:,:]\n","        target=self.target_big[ind1:ind2]\n","        \n","        # Get the events that will be extracted from the file\n","        #ind2=ind+self.batch_size\n","        \n","        data=data.unsqueeze(dim=3)\n","        target=target.unsqueeze(dim=1)\n","        \n","        #Only for conv with modulus\n","        data=data.transpose(1,3)\n","        data=data.transpose(2,3)\n","        \n","        \n","        #target[:,0]=torch.tensor(f['y'][ind:ind2,0])\n","        target=torch.arccos(target)\n","    \n","        \n","        return data.float(),target.float()"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"nY59_IMwmWtC","executionInfo":{"status":"ok","timestamp":1624481352054,"user_tz":-120,"elapsed":2,"user":{"displayName":"antonio aslan suarez","photoUrl":"","userId":"04350384891875098633"}}},"source":["def init_data(percentage,batch):\n","        # Save for exporting\n","        percentage=percentage\n","        batch_size=batch\n","        # Initialize the datasets\n","        print(\"Train dataset:\")\n","        training_data = EventsData(data_dir='Mod_full_dist/train_data_fixed/', per=percentage, batch_size=batch);\n","        print()\n","\n","        print(\"Validation dataset:\")\n","        validation_data=EventsData(data_dir='Mod_full_dist/validation_data_fixed/', per=percentage, batch_size=batch);\n","        print()\n","\n","        print(\"Test dataset:\")\n","        test_data=EventsData(data_dir='Mod_full_dist/test_data_fixed/', per=percentage, batch_size=batch);\n","        print()\n","\n","        return training_data,validation_data,test_data"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-Rdkj7yWm6nz","executionInfo":{"status":"ok","timestamp":1624481632373,"user_tz":-120,"elapsed":278867,"user":{"displayName":"antonio aslan suarez","photoUrl":"","userId":"04350384891875098633"}},"outputId":"21ecf956-41c6-42bb-a74b-2a673749d9e6"},"source":["batch_size_used = 1000 # Has to be fixed\n","percentage_used = 100\n","\n","train,val,test = init_data(percentage_used,batch_size_used)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Train dataset:\n","Mod_full_dist/train_data_fixed/train_dataset_0.hdf5\n","Reading Mod_full_dist/train_data_fixed/ with 862 files.\n","There are 862000 events.\n","In 862 separate files.\n","Each file containing 1000 events.\n","In 862 iterations\n","The real number of events is: 862000\n","\n","Validation dataset:\n","Mod_full_dist/validation_data_fixed/validation_dataset_0.hdf5\n","Reading Mod_full_dist/validation_data_fixed/ with 247 files.\n","There are 247000 events.\n","In 247 separate files.\n","Each file containing 1000 events.\n","In 247 iterations\n","The real number of events is: 247000\n","\n","Test dataset:\n","Mod_full_dist/test_data_fixed/test_dataset_0.hdf5\n","Reading Mod_full_dist/test_data_fixed/ with 120 files.\n","There are 120000 events.\n","In 120 separate files.\n","Each file containing 1000 events.\n","In 120 iterations\n","The real number of events is: 120000\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"C6uzAaW2is8t"},"source":["## Declaramos la arquitectura de nuestra red, tiene que ser exactamente igual a que vamos a evaluar"]},{"cell_type":"code","metadata":{"id":"bQZd5ThNSLCK","executionInfo":{"status":"ok","timestamp":1624481632375,"user_tz":-120,"elapsed":13,"user":{"displayName":"antonio aslan suarez","photoUrl":"","userId":"04350384891875098633"}}},"source":["# This class will contain the NN arquitecture, it will be pushed to the GPU\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        \n","        kernel_size=(2,10)\n","        \n","        #conv layer, sees 25x161x1 tensor\n","        self.conv1 = nn.Conv2d(1, 16, kernel_size,padding=1)\n","        self.conv2 = nn.Conv2d(16, 32, kernel_size,padding=1)\n","        self.conv3 = nn.Conv2d(32, 64, kernel_size,padding=1)\n","        self.conv4 = nn.Conv2d(64, 128, kernel_size,padding=1)\n","\n","        \n","        self.pool = nn.MaxPool2d((2, 2))\n","        input_flatten = 6656\n","        hidden_1 = 600\n","        hidden_2 = 200\n","        hidden_3 = 50\n","        # linear layer (784 -> hidden_1)\n","        self.fc1 = nn.Linear(input_flatten, hidden_1)\n","        # linear layer (n_hidden -> hidden_2)\n","        self.fc2 = nn.Linear(hidden_1, hidden_2)\n","        \n","        self.fc3 = nn.Linear(hidden_2, hidden_3)\n","        self.mu = nn.Linear(hidden_3, 1)\n","        self.sigma = nn.Linear(hidden_3,1)\n","        \n","        # dropout layer (p=0.2)\n","        # dropout prevents overfitting of data\n","        self.dropout = nn.Dropout(0.25)\n","\n","\n","    def forward(self, x):\n","        input_flatten = 6656\n","        # add sequence of convolutional and max pooling layers\n","        x = (F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = self.pool(F.relu(self.conv3(x)))\n","        x = self.pool(F.relu(self.conv4(x)))\n","   \n","        x = x.reshape(-1, input_flatten)\n","        # add hidden layer, with relu activation function\n","        x = F.relu(self.fc1(x))\n","        x = self.dropout(x)\n","        x = F.relu(self.fc2(x))\n","        x = self.dropout(x)\n","        x = F.relu(self.fc3(x))\n","        x = self.dropout(x)\n","        \n","        mu = self.mu(x)\n","        sigma = torch.exp(self.sigma(x))\n","        return mu,sigma"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hyP1Q4G2hVGW"},"source":["## Pasamos los datos a la red, podemos seleccionar:\n","* La carpeta donde está el archivo\n","* El nombre del archivo que vamos a cargar\n","* El dispositivo donde vamos a correr la red: cuda o cpu\n","\n","También declaramos las variables en RAM donde guardaremos los resultados de los diferentes datasets"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hg9WmPRKTbqi","executionInfo":{"status":"ok","timestamp":1624481639883,"user_tz":-120,"elapsed":7516,"user":{"displayName":"antonio aslan suarez","photoUrl":"","userId":"04350384891875098633"}},"outputId":"f1b577ac-b295-4388-b4f6-ef9e4b8b0243"},"source":["# Check if cuda is available and set as default device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","net=Net()\n","print(net)\n","net.cuda()\n","# Cargar aqui el archivo\n","net.load_state_dict(torch.load('Conv7_MDN/Conv7_MDNH_1000_Rad_exp_2.pt'))\n","net.eval()\n","\n","# This is where the values will be stored\n","results_train=np.zeros([train.get_len(),2])\n","results_validation=np.zeros([val.get_len(),2])\n","results_test=np.zeros([test.get_len(),2])"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Net(\n","  (conv1): Conv2d(1, 16, kernel_size=(2, 10), stride=(1, 1), padding=(1, 1))\n","  (conv2): Conv2d(16, 32, kernel_size=(2, 10), stride=(1, 1), padding=(1, 1))\n","  (conv3): Conv2d(32, 64, kernel_size=(2, 10), stride=(1, 1), padding=(1, 1))\n","  (conv4): Conv2d(64, 128, kernel_size=(2, 10), stride=(1, 1), padding=(1, 1))\n","  (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n","  (fc1): Linear(in_features=6656, out_features=600, bias=True)\n","  (fc2): Linear(in_features=600, out_features=200, bias=True)\n","  (fc3): Linear(in_features=200, out_features=50, bias=True)\n","  (mu): Linear(in_features=50, out_features=1, bias=True)\n","  (sigma): Linear(in_features=50, out_features=1, bias=True)\n","  (dropout): Dropout(p=0.25, inplace=False)\n",")\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7KMM9FntjjI9"},"source":["## Sacamos los resultados de todos los datasets y los guardamos en sus corresponientes variables"]},{"cell_type":"code","metadata":{"id":"M8cRgDjbj273","executionInfo":{"status":"ok","timestamp":1624481974475,"user_tz":-120,"elapsed":35227,"user":{"displayName":"antonio aslan suarez","photoUrl":"","userId":"04350384891875098633"}}},"source":["for batch in range(train.get_iter()):\n","    # Get the data\n","    data,target = train.get_batch(batch)\n","    # move tensors to GPU if CUDA is available\n","    data = data.cuda()\n","    # Get the results from the foward pass to the CPU \n","    # And get it as an numpy matrix\n","    mu,sigma = net(data)\n","    \n","    # Get from tensors to numpy\n","    output = mu.cpu().detach().numpy()\n","    sigma = sigma.detach().cpu().numpy()\n","\n","    output.shape=(batch_size_used)\n","    sigma.shape=(batch_size_used)\n","\n","    # Get Z components from the Zenith angle (in radians)\n","    zeta = np.cos(output)\n","    zigma = sigma*np.sin(output)\n","\n","    # Append to the historical value\n","    results_train[(batch*1000):((batch+1)*1000),0]=zeta\n","    results_train[(batch*1000):((batch+1)*1000),1]=zigma\n"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"IQ6wvEjzlP5H","executionInfo":{"status":"ok","timestamp":1624481984403,"user_tz":-120,"elapsed":9937,"user":{"displayName":"antonio aslan suarez","photoUrl":"","userId":"04350384891875098633"}}},"source":["for batch in range(val.get_iter()):\n","    # Get the data\n","    data,target = val.get_batch(batch)\n","    # move tensors to GPU if CUDA is available\n","    data = data.cuda()\n","    # Get the results from the foward pass to the CPU \n","    # And get it as an numpy matrix\n","    mu,sigma = net(data)\n","    \n","    # Get from tensors to numpy\n","    output = mu.cpu().detach().numpy()\n","    sigma = sigma.detach().cpu().numpy()\n","\n","    output.shape=(batch_size_used)\n","    sigma.shape=(batch_size_used)\n","\n","    # Get Z components from the Zenith angle (in radians)\n","    zeta = np.cos(output)\n","    zigma = sigma*np.sin(output)\n","\n","    # Append to the historical value\n","    results_validation[(batch*1000):((batch+1)*1000),0]=zeta\n","    results_validation[(batch*1000):((batch+1)*1000),1]=zigma"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"eYBCx4f4laV9","executionInfo":{"status":"ok","timestamp":1624481989248,"user_tz":-120,"elapsed":4848,"user":{"displayName":"antonio aslan suarez","photoUrl":"","userId":"04350384891875098633"}}},"source":["for batch in range(test.get_iter()):\n","    # Get the data\n","    data,target = test.get_batch(batch)\n","    # move tensors to GPU if CUDA is available\n","    data = data.cuda()\n","    # Get the results from the foward pass to the CPU \n","    # And get it as an numpy matrix\n","    mu,sigma = net(data)\n","    \n","    # Get from tensors to numpy\n","    output = mu.cpu().detach().numpy()\n","    sigma = sigma.detach().cpu().numpy()\n","\n","    output.shape=(batch_size_used)\n","    sigma.shape=(batch_size_used)\n","\n","    # Get Z components from the Zenith angle (in radians)\n","    zeta = np.cos(output)\n","    zigma = sigma*np.sin(output)\n","\n","    # Append to the historical value\n","    results_test[(batch*1000):((batch+1)*1000),0]=zeta\n","    results_test[(batch*1000):((batch+1)*1000),1]=zigma\n"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qDMBJn4Kl7VN"},"source":["## Guardamos los resultados en un archivo .hdf5 "]},{"cell_type":"code","metadata":{"id":"_W2NrBXQCHGQ","executionInfo":{"status":"ok","timestamp":1624482071333,"user_tz":-120,"elapsed":428,"user":{"displayName":"antonio aslan suarez","photoUrl":"","userId":"04350384891875098633"}}},"source":["# Abrimos el archivo\n","f = h5py.File(\"Z_results_zenith.hdf5\", \"w\")\n","# Guardamos las variables\n","f.create_dataset('Train_Z', data=results_train)\n","f.create_dataset('Val_Z', data=results_validation)\n","f.create_dataset('Test_Z', data=results_test)\n","# Cerramos el archivo\n","f.close()"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xt301-nFlmF0"},"source":[""],"execution_count":null,"outputs":[]}]}