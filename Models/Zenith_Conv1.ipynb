{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the variables for the behaviour of the script\n",
    "\n",
    "* *batch_size_used*: Defines the batch size used by the dataloader.\n",
    "* *percentage_used*: Defines the percentage of the total dataset that will be used.\n",
    "* *folder_name*: Name of the folder where the training results will be saved.\n",
    "* *net_name*: Identifier of the network.\n",
    "* *n_nets*: Number of networks that will be trained.\n",
    "* *n_epochs*: Maximum number of epochs, early stopping is activated.\n",
    "* *test_interval*: Defines how many times the network will calculate the MAE and RMSE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_used = 1000 # Has to be fixed\n",
    "percentage_used = 100\n",
    "folder_name = \"Conv1_MDN\"\n",
    "net_name = \"Conv13\"\n",
    "n_epochs =  100 # Has to be fixed\n",
    "n_nets = 3\n",
    "test_interval=5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yjjpgFssTJsy"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import shutil\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import glob\n",
    "import gzip\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import h5py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare and initialize the dataloader\n",
    "1. Declare the class EventsData, which will act as the model's dataloader.\n",
    "2. Declare the function that will initialize the three dataloader.\n",
    "3. Initialize the three dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hMzALyOVTMcp"
   },
   "outputs": [],
   "source": [
    "class EventsData(Dataset):\n",
    "    def __init__(self,data_dir,per=100,batch_size=500):\n",
    "        # Save the directory of the data\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "        # Get the names of the files\n",
    "        self.names=glob.glob(str(data_dir)+'*.hdf5')\n",
    "        # Number of files in the dir\n",
    "        self.size_dir=len(self.names)\n",
    "        \n",
    "        # Set the files acording to the percentage\n",
    "        self.size_dir=math.ceil(len(self.names)*(per/100))\n",
    "        self.names=self.names[0:self.size_dir]\n",
    "        #random.shuffle(self.names)\n",
    "        \n",
    "        # Get the number of events per file\n",
    "        print(self.names[1])\n",
    "        f = h5py.File(self.names[0],'r')\n",
    "        self.size_file=f['y'].shape[0]\n",
    "        \n",
    "        # Get the total number of events\n",
    "        self.total_events=0\n",
    "        for name in self.names:\n",
    "            f = h5py.File(name,'r')\n",
    "            y = f['y']\n",
    "            self.total_events+=y.shape[0]\n",
    "            \n",
    "        # Load the whole dataset into the RAM\n",
    "        self.data_big = torch.zeros(self.total_events,25,161)\n",
    "        self.target_big = torch.zeros(self.total_events)\n",
    "\n",
    "        print(\"Reading \"+str(self.data_dir)+\" with \"+str(self.size_dir)+\" files.\")\n",
    "        for a in range(len(self.names)):\n",
    "            f = h5py.File(self.names[a],'r')\n",
    "            self.data_big[(a*1000):(((a+1)*1000))]=torch.tensor(f['X1'][:,:,:,0])\n",
    "            self.target_big[(a*1000):(((a+1)*1000))]=torch.tensor(f['y'][:,0])\n",
    "            self.target_big[(a*1000):(((a+1)*1000))].size()\n",
    "            \n",
    "        # Number of iterations to finish the dataset\n",
    "        self.batch_size=batch_size\n",
    "        self.iters=math.floor(self.total_events/batch_size)\n",
    "        self.iters_per_file= math.floor(self.size_file/batch_size)\n",
    "        self.real_events=self.batch_size*self.iters\n",
    "\n",
    "        print(\"There are \"+str(self.total_events)+\" events.\")\n",
    "        print(\"In \"+str(self.size_dir)+\" separate files.\")\n",
    "        print(\"Each file containing \"+str(self.size_file)+\" events.\")\n",
    "        print(\"In \"+str(self.iters)+\" iterations\")\n",
    "        print(\"The real number of events is: \"+str(self.real_events))\n",
    "        \n",
    "\n",
    "\n",
    "    def get_len(self):\n",
    "        return self.real_events\n",
    "    \n",
    "    def get_iter(self):\n",
    "        # Returns the number of iteracions og getitem to finish the dataset\n",
    "        return self.iters\n",
    "\n",
    "    def get_batch(self, idx):\n",
    "        # Get the file that shall be opened\n",
    "        ind1=idx*self.batch_size\n",
    "        ind2=((idx+1)*self.batch_size)\n",
    "        \n",
    "        #print(ind1)\n",
    "        #print(ind2)\n",
    "        \n",
    "        data=self.data_big[ind1:ind2,:,:]\n",
    "        target=self.target_big[ind1:ind2]\n",
    "        \n",
    "        # Get the events that will be extracted from the file\n",
    "        #ind2=ind+self.batch_size\n",
    "        \n",
    "        data=data.unsqueeze(dim=3)\n",
    "        target=target.unsqueeze(dim=1)\n",
    "        \n",
    "        #Only for conv with modulus\n",
    "        data=data.transpose(1,3)\n",
    "        data=data.transpose(2,3)\n",
    "        \n",
    "        \n",
    "        #target[:,0]=torch.tensor(f['y'][ind:ind2,0])\n",
    "        target=torch.arccos(target)\n",
    "    \n",
    "        \n",
    "        return data.float(),target.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nY59_IMwmWtC"
   },
   "outputs": [],
   "source": [
    "def init_data(percentage,batch):\n",
    "        # Save for exporting\n",
    "        percentage=percentage\n",
    "        batch_size=batch\n",
    "        # Initialize the datasets\n",
    "        print(\"Train dataset:\")\n",
    "        training_data = EventsData(data_dir='Mod_full_dist/train_data_fixed/', per=percentage, batch_size=batch);\n",
    "        print()\n",
    "\n",
    "        print(\"Validation dataset:\")\n",
    "        validation_data=EventsData(data_dir='Mod_full_dist/validation_data_fixed/', per=percentage, batch_size=batch);\n",
    "        print()\n",
    "\n",
    "        print(\"Test dataset:\")\n",
    "        test_data=EventsData(data_dir='Mod_full_dist/test_data_fixed/', per=percentage, batch_size=batch);\n",
    "        print()\n",
    "\n",
    "        return training_data,validation_data,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 261298,
     "status": "ok",
     "timestamp": 1624952295722,
     "user": {
      "displayName": "antonio aslan suarez",
      "photoUrl": "",
      "userId": "04350384891875098633"
     },
     "user_tz": -120
    },
    "id": "-Rdkj7yWm6nz",
    "outputId": "5c075665-17cb-432d-a4b9-5c33528c0562"
   },
   "outputs": [],
   "source": [
    "train,val,test = init_data(percentage_used,batch_size_used)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare the model's architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bQZd5ThNSLCK"
   },
   "outputs": [],
   "source": [
    "# This class will contain the NN architecture, it will be pushed to the GPU\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        kernel_size=(5,5)\n",
    "        \n",
    "        #conv layer, sees 25x161x1 tensor\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size,padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 36, kernel_size,padding=1)\n",
    "        \n",
    "\n",
    "        \n",
    "        self.pool = nn.MaxPool2d((2, 2))\n",
    "        input_flatten = 5472\n",
    "        hidden_1 = 500\n",
    "        hidden_2 = 200\n",
    "        hidden_3 = 50\n",
    "        # linear layer (784 -> hidden_1)\n",
    "        self.fc1 = nn.Linear(input_flatten, hidden_1)\n",
    "        # linear layer (n_hidden -> hidden_2)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden_2, hidden_3)\n",
    "        self.mu = nn.Linear(hidden_3, 1)\n",
    "        self.sigma = nn.Linear(hidden_3,1)\n",
    "        \n",
    "        # dropout layer (p=0.2)\n",
    "        # dropout prevents overfitting of data\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_flatten = 5472\n",
    "        # add sequence of convolutional and max pooling layers\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        \n",
    "   \n",
    "        x = x.reshape(-1, input_flatten)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        mu = self.mu(x)\n",
    "        sigma = torch.exp(self.sigma(x))\n",
    "        return mu,sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the custom loss and the Net_Info class\n",
    "\n",
    "The Net_Info class will run the training of the model, saving all the information for a more convient control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "im6PmYOgR0Q6"
   },
   "outputs": [],
   "source": [
    "# Nueva función de coste\n",
    "def custom_loss(target,mu,sigma):\n",
    "    # Create the normal distribution\n",
    "    #print(mu[1])\n",
    "    dist = torch.distributions.Normal(loc=mu, scale=sigma)\n",
    "    # Obtain the -PDF and reduce it to the mean\n",
    "    # Shall return a real number only\n",
    "    loss = torch.mean(-dist.log_prob(target))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0uNQbYDjTQ3B"
   },
   "outputs": [],
   "source": [
    "def weight_reset(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()\n",
    "\n",
    "# This class will contain a NN model and all of its functions and data \n",
    "# It won't be pushed to the GPU\n",
    "class Net_Info():\n",
    "    # Constructor for the predictor\n",
    "    # Data regarding the NN shall be passed here\n",
    "    def __init__(self,model,folder,name):\n",
    "        # Route to save the model\n",
    "        self.folder=folder\n",
    "        self.name=name\n",
    "\n",
    "        # Set the optimizer and loss functions\n",
    "        self.optimizer=optim.SGD(model.parameters(),lr=0.002)\n",
    "        #self.loss_function=nn.L1Loss()\n",
    "        \n",
    "    # Function to set the model name and folder\n",
    "    # Used to save the name in it\n",
    "    def location(self,name,folder):\n",
    "        self.folder=folder\n",
    "        self.model_name=name\n",
    "\n",
    "    def init_data(self,percentage,batch):\n",
    "        # Save for exporting\n",
    "        self.percentage=percentage\n",
    "        self.batch_size=batch\n",
    "        \n",
    "\n",
    "    # Save all the data from the class, except the arquitecture of the net\n",
    "    def save_params(self,model):\n",
    "        # Open the text file\n",
    "        file1=open(str(self.folder)+\"/\"+str(self.name)+\"_params.txt\",\"w+\")\n",
    "        file1.write(\"Data Info:\\n\")\n",
    "        file1.write(str(self.percentage)+\"\\n\"+str(self.batch_size)+\"\\n\"+str(self.n_epochs)+\"\\n\")\n",
    "        file1.write(\"Last Val Info:\\n\")\n",
    "        file1.write(str(self.valid_loss_min)+\"\\n\")\n",
    "        file1.write(\"Last Errors:\\n\")\n",
    "        file1.write(str(self.MAE)+\"\\n\"+ str(self.RMSE)+\"\\n\")\n",
    "        file1.write(\"Loss and Optimier\\n\")\n",
    "        #file1.write(str(self.loss_function)+\"\\n\")\n",
    "        file1.write(str(self.optimizer))\n",
    "        file1.close()\n",
    "        file2=open(str(self.folder)+\"/\"+str(self.name)+\"_architecture.txt\",\"w+\")\n",
    "        file2.write(str(model))\n",
    "        file2.close()\n",
    "\n",
    "    def plot_loss(self):\n",
    "        plt.figure(figsize=(10,10),dpi=800)\n",
    "        plt.plot(self.train_loss_temp[:self.n_epochs],linewidth=3.0,color=\"r\")\n",
    "        plt.plot(self.valid_loss_temp[:self.n_epochs],linewidth=3.0,color=\"g\")\n",
    "        plt.title('{} | MAE = {:.3f} | RMSE = {:.3f}'.format(self.name,self.MAE,self.RMSE))\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.legend(['Training Loss','Validation Loss'], loc = 'upper left')\n",
    "        # plt.show()\n",
    "        plt.savefig(str(self.folder)+\"/\"+str(self.name)+\"_loss.jpg\")\n",
    "\n",
    "    def plot_pred(self):\n",
    "        plt.figure(figsize=(5,5),dpi=100)\n",
    "        plt.scatter(self.target_temp,self.output_temp)\n",
    "        plt.xlabel('True Values ')\n",
    "        plt.ylabel('Predictions ')\n",
    "        plt.axis('equal')\n",
    "        plt.axis('square')\n",
    "        plt.title(\"Target vs. Prediction values of \"+str(self.name))\n",
    "        plt.savefig(str(self.folder)+\"/\"+str(self.name)+\"_scatter.jpg\")\n",
    "        plt.close();\n",
    "      \n",
    "        \n",
    "\n",
    "    def train_model(self,net,n_epochs,test_inter,train,val,test,valid_loss_min=np.Inf):\n",
    "        self.n_epochs = n_epochs\n",
    "        # If valid loss is not inf, the load the model\n",
    "        if valid_loss_min>1000:\n",
    "            self.valid_loss_min = np.Inf # track change in validation loss\n",
    "        else:\n",
    "            self.valid_loss_min = valid_loss_min\n",
    "            net.load_state_dict(torch.load(str(self.folder)+'/'+str(self.name)+'.pt'))\n",
    "            \n",
    "\n",
    "        self.train_loss_temp = np.zeros([n_epochs,1])\n",
    "        self.valid_loss_temp = np.zeros([n_epochs,1])\n",
    "        # If this number goes to zero, the training will stop\n",
    "        last_save=10\n",
    "        for epoch in range(1, n_epochs+1):\n",
    "            # keep track of training and validation loss\n",
    "            train_loss = 0.0\n",
    "            valid_loss = 0.0\n",
    "\n",
    "\n",
    "\n",
    "            # train the net #\n",
    "            net.train()\n",
    "            for batch in range(train.get_iter()):\n",
    "                # Get the data\n",
    "                data,target=train.get_batch(batch)\n",
    "                # move tensors to GPU if CUDA is available\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "                # clear the gradients of all optimized variables\n",
    "                self.optimizer.zero_grad()\n",
    "                # forward pass: compute predicted outputs by passing inputs to the net\n",
    "                mu,sigma = net(data)\n",
    "                # calculate the batch loss\n",
    "                loss = custom_loss(target,mu,sigma)\n",
    "                # backward pass: compute gradient of the loss with respect to net parameters\n",
    "                loss.backward()\n",
    "                # perform a single optimization step (parameter update)\n",
    "                self.optimizer.step()\n",
    "                # update training loss\n",
    "                # and reboot if nan\n",
    "                if math.isnan(loss.item()) == True:\n",
    "                    net.apply(weight_reset)\n",
    "                    print(\"Reseteo insacioso\")\n",
    "                train_loss += loss.item()\n",
    "\n",
    "\n",
    "            # validate the net #\n",
    "            net.eval()\n",
    "            for batch in range(val.get_iter()):\n",
    "                # Get the data\n",
    "                data,target=val.get_batch(batch)   \n",
    "                # move tensors to GPU if CUDA is available\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "                # forward pass: compute predicted outputs by passing inputs to the net\n",
    "                mu,sigma = net(data)\n",
    "                # calculate the batch loss\n",
    "                loss = custom_loss(target,mu,sigma)\n",
    "                # update average validation loss \n",
    "                valid_loss += loss.item()\n",
    "            #el data,target\n",
    "            # calculate average losses\n",
    "            train_loss = train_loss/train.get_len()\n",
    "            valid_loss = valid_loss/val.get_len()\n",
    "\n",
    "            # Append the losses to the historical ones\n",
    "            self.train_loss_temp[epoch-1]=train_loss\n",
    "            self.valid_loss_temp[epoch-1]=valid_loss            \n",
    "\n",
    "            # print training/validation statistics \n",
    "            print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "                epoch, train_loss, valid_loss))\n",
    "            # Save the net if the loss goes down\n",
    "            # Show also the % of error down\n",
    "            if valid_loss <= self.valid_loss_min:\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving net ...'.format(\n",
    "                self.valid_loss_min,\n",
    "                valid_loss))\n",
    "                torch.save(net.state_dict(), str(self.folder)+'/'+str(self.name)+'.pt')\n",
    "                # Show by how much the validation loss has dropped\n",
    "                valid_percent=((self.valid_loss_min-valid_loss)/self.valid_loss_min)*100\n",
    "                print('Validation loss has dropped {:.2f}%'.format(valid_percent))\n",
    "                self.valid_loss_min = valid_loss\n",
    "                if valid_percent>=1:\n",
    "                    last_save=15\n",
    "                    print(\"Keep training\")\n",
    "                else:\n",
    "                    last_save += 1\n",
    "            else:\n",
    "                last_save -= 1;\n",
    "                \n",
    "            # Show iterations to stop\n",
    "            print('Training will stop in '+str(last_save)+\" epochs ...\")\n",
    "            print(\"\")\n",
    "            \n",
    "            # Check the test error and the \n",
    "            if (epoch%test_interval)==0:\n",
    "                # self.target_temp=np.zeros([self.test_data.get_len(),1])\n",
    "                # self.output_temp=np.zeros([self.test_data.get_len(),1])\n",
    "                # Init the errors\n",
    "                MAE=0\n",
    "                MSE=0\n",
    "                for batch in range(test.get_iter()):\n",
    "                    # Get the data\n",
    "                    data,target=test.get_batch(batch)\n",
    "                    # move tensors to GPU if CUDA is available\n",
    "                    data = data.cuda()\n",
    "                    # Get the results from the foward pass to the CPU \n",
    "                    # And get it as an numpy matrix\n",
    "                    mu,sigma = net(data)#.cpu().detach().numpy()\n",
    "                    target=target.numpy()*(180.0/math.pi)\n",
    "                    output = mu.cpu().detach().numpy()*(180.0/math.pi)\n",
    "                    # calculate the batch loss\n",
    "                    MAE += np.sum(np.abs(output-target))\n",
    "                    MSE += np.sum((output-target)**2)\n",
    "                    # print(output)\n",
    "                    # Append to the historical value\n",
    "                    # self.target_temp[(batch*self.batch_size):((batch+1)*self.batch_size)]=target\n",
    "                    # self.output_temp[(batch*self.batch_size):((batch+1)*self.batch_size)]=output\n",
    "\n",
    "                # self.plot_pred()\n",
    "                # calculate average losses\n",
    "                self.MAE = MAE/test.get_len()\n",
    "                self.RMSE = np.sqrt(MSE/test.get_len())\n",
    "                    \n",
    "                print(\"The MAE is \"+str(self.MAE))\n",
    "                print(\"The RMSE is \"+str(self.RMSE))\n",
    "\n",
    "            # Check if the net is not progressing\n",
    "            if last_save<=0:\n",
    "                print(\"Early stopping\")\n",
    "                self.n_epochs=epoch\n",
    "                break\n",
    "        # Compute the test error for the best performing model\n",
    "        # Load the model\n",
    "        net.load_state_dict(torch.load(str(self.folder)+'/'+str(self.name)+'.pt'))\n",
    "        MAE=0\n",
    "        MSE=0\n",
    "        for batch in range(test.get_iter()):\n",
    "            # Get the data\n",
    "            data,target=test.get_batch(batch)\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            data = data.cuda()\n",
    "            # Get the results from the foward pass to the CPU \n",
    "            # And get it as an numpy matrix\n",
    "            mu,sigma = net(data)\n",
    "            target=target.numpy()*(180.0/math.pi)\n",
    "            output = mu.cpu().detach().numpy()*(180.0/math.pi)\n",
    "            # calculate the batch loss\n",
    "            MAE += np.sum(np.abs(output-target))\n",
    "            MSE += np.sum((output-target)**2)\n",
    "            # print(output)\n",
    "            # Append to the historical value\n",
    "            # self.target_temp[(batch*self.batch_size):((batch+1)*self.batch_size)]=target\n",
    "            # self.output_temp[(batch*self.batch_size):((batch+1)*self.batch_size)]=output\n",
    "\n",
    "        # self.plot_pred()\n",
    "        # calculate average losses\n",
    "        self.MAE = MAE/test.get_len()\n",
    "        self.RMSE = np.sqrt(MSE/test.get_len())\n",
    "\n",
    "        print(\"The MAE is \"+str(self.MAE))\n",
    "        print(\"The RMSE is \"+str(self.RMSE))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1InR3ga7_WQwcDVtbhj4-CZmtOuo1u45f"
    },
    "executionInfo": {
     "elapsed": 2106765,
     "status": "ok",
     "timestamp": 1624974819744,
     "user": {
      "displayName": "antonio aslan suarez",
      "photoUrl": "",
      "userId": "04350384891875098633"
     },
     "user_tz": -120
    },
    "id": "Hg9WmPRKTbqi",
    "outputId": "627746b2-1ee7-46e3-c2f0-2518823632c6"
   },
   "outputs": [],
   "source": [
    "# Try to create the folder name\n",
    "try:\n",
    "    os.mkdir(str(folder_name))\n",
    "except FileExistsError:\n",
    "    print(str(folder_name)+ \" directory already exists\")\n",
    "\n",
    "# Copy the modified python script with the net name into the subfolder\n",
    "shutil.copy2(sys.argv[0],str(folder_name)+\"/\"+str(net_name)+\".py\")  \n",
    "\n",
    "# Check if cuda is available and set as default device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using decive \"+str(device))\n",
    "# Run all the models\n",
    "# Run all the learning rates\n",
    "\n",
    "for b in range(n_nets):\n",
    "    # initialize the NN\n",
    "    new_name=str(net_name)+\"_\"+str(b)\n",
    "    print(\"Running \"+str(new_name))\n",
    "    net=Net()\n",
    "    print(net)\n",
    "    net.cuda()\n",
    "    # Initialize the Net_Info object\n",
    "    Model=Net_Info(net,folder_name,new_name)\n",
    "    Model.optimizer=optim.Adadelta(net.parameters())\n",
    "    # Initialize the datasets\n",
    "    Model.init_data(percentage_used,batch_size_used)\n",
    "    # This next may be inside of a function\n",
    "    Model.train_model(net,n_epochs,test_interval,train,val,test)\n",
    "    # Save the model\n",
    "    Model.save_params(net)\n",
    "    Model.plot_loss() \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMXfQDfXBXNMcILRThJkusv",
   "machine_shape": "hm",
   "mount_file_id": "1U37gNKJrDYK7UQVfNDji5Rg6LOQxkZOZ",
   "name": "Conv_1_MDNH_exp.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
